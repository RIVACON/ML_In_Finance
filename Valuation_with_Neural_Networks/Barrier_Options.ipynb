{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/logo.png\" alt=\"Your Image\" style=\"width: 500px; height: auto\"></center>\n",
    "\n",
    "# Valuation of barrier options via Neural Networks\n",
    "\n",
    "In this notebook, we study the problem of pricing barrier options using a neural network model. The results are compared with values computed from the closed formula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as si\n",
    "import tensorflow as tf\n",
    "from tensorflow import Variable\n",
    "from tensorflow.keras import backend, optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, iter_min = 0, iter_max = -1):\n",
    "    \"\"\"Plot loss during network training on training and validation data\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['loss'][iter_min:iter_max],'-x', label='training')\n",
    "    plt.plot(history.history['val_loss'][iter_min:iter_max],'-x', label='validation')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('#iterations')\n",
    "    plt.ylabel('loss function value')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_loss_function(model, x, y, n_samples=30, random_state=42, start=-0.1, end=0.1, stepsize=0.001):\n",
    "    \"\"\"Plot projection of the network loss function along straight lines through the current network variables\n",
    "    This method may be helpful to get an impression if the training has slowed down due \n",
    "    to a saddlepoint or maybe because a real local minima has been reached\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    weights = [ np.copy(x) for x in model.get_weights() ]\n",
    "    \n",
    "    steps = np.arange(start,end,stepsize)\n",
    "    loss_values = np.empty(steps.shape)\n",
    "    for sample in range(n_samples):\n",
    "        direction = []\n",
    "        for w in weights:\n",
    "            direction.append( np.random.uniform(0, 1, size = w.shape) )\n",
    "        for i in range(steps.shape[0]):\n",
    "            w = []\n",
    "            for j in range(len(weights)):\n",
    "                new_weights = np.copy(weights[j])\n",
    "                new_weights += steps[i]*direction[j]\n",
    "                w.append(new_weights)\n",
    "            model.set_weights(w)\n",
    "            loss_values[i] = model.evaluate(x,y, verbose=0)\n",
    "        plt.plot(steps, loss_values)\n",
    "    model.set_weights(weights)  \n",
    "    plt.ylabel('loss')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.axvline(x=0.0)\n",
    "      \n",
    "def analyze_model(model, history, x, y):\n",
    "    \"\"\"This method plots the approximation errors on training and test data set.\n",
    "    \"\"\"\n",
    "    print('Loss function on training data: '+ str(history.history['loss'][-1]))\n",
    "    print('Loss function on validation data: '+ str(history.history['val_loss'][-1]))\n",
    "\n",
    "    y_pred = model.predict(x).squeeze()\n",
    "    \n",
    "    plt.subplots(figsize=(25,12))\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.subplot(2,2,1)\n",
    "    plot_loss(history)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()    \n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(x[:,0], y-y_pred, '.', label='y-y_pred')\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('y-y_pred')\n",
    "    plt.tight_layout()\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.plot(x[:,1], y-y_pred, '.', label='y-y_pred')\n",
    "    plt.xlabel('T')\n",
    "    plt.ylabel('y-y_pred')\n",
    "    plt.tight_layout()\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.plot(x[:,2], y-y_pred, '.', label='y-y_pred')\n",
    "    plt.xlabel('vol')\n",
    "    plt.ylabel('y-y_pred')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barrier options\n",
    "\n",
    "For one-touch barrier options (cf., https://en.wikipedia.org/wiki/Barrier_option and https://perswww.kuleuven.be/~u0009713/ScSiTi03.pdf), we focus on the following two types:\n",
    "\n",
    "First, we consider the case of an **Up and Out CALL** barrier option. It is worthless unless its maximum remains below some high barrier B, in which case it retains the structure of a European call option with strike K. Its price is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "UOB_{Call} = e^{- r (T-t)} \\mathbb{E}^{\\mathbb{Q}} \\bigl[ (S_T - K)^+ \\mathbb{1}_{\\{M_T < B\\}} \\, \\big| \\, S_t=s \\bigr].\n",
    "\\end{equation}\n",
    "\n",
    "with\n",
    "\n",
    "$$ M_T = \\max_{0\\leq t \\leq T} S_t. $$\n",
    "\n",
    "and the risk neutral measure $\\mathbb{Q}$.\n",
    "\n",
    "\n",
    "Next, we consider the case of a **Down and In CALL** barrier option, which is like a \"normal\" European call option, if its minimum went below some \"low barrier\". If this barrier was never reached during the life-time of the option, the option remains worthless. Its prce is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "DIB_{Call} = e^{- r (T-t)} \\mathbb{E}^{\\mathbb{Q}} \\bigl[ (S_T - K)^+ \\mathbb{1}_{\\{\\tilde M_T \\leq B\\}} \\, \\big| \\, S_t=s \\bigr].\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\tilde M_T = \\min_{0\\leq t \\leq T} S_t. $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and plot data\n",
    "\n",
    "We create input data for the training of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_and_out_call(S0, K, T, r, sigma, B):\n",
    "    \"\"\" \n",
    "    Compute a call price of a up and out barrier option (closed form solution)\n",
    "    \"\"\"\n",
    "\n",
    "    d1 = lambda t, s: 1 / (sigma * np.sqrt(t)) * (np.log(s) + (r + sigma**2 / 2) * t)\n",
    "    d2 = lambda t, s: 1 / (sigma * np.sqrt(t)) * (np.log(s) + (r - sigma**2 / 2) * t)\n",
    "\n",
    "    call = (\n",
    "    S0 * (si.norm.cdf(d1(T, S0 / K)) - si.norm.cdf(d1(T, S0 / B)))\n",
    "    - np.exp(-r * T) * K * (si.norm.cdf(d2(T, S0 / K)) - si.norm.cdf(d2(T, S0 / B)))\n",
    "    - B * (S0 / B) ** (-2 * r / sigma**2) * (si.norm.cdf(d1(T, B**2 / (S0 * K))) - si.norm.cdf(d1(T, B / S0)))\n",
    "    + np.exp(-r * T)\n",
    "    * K\n",
    "    * (S0 / B) ** (-2 * r / sigma**2 + 1)\n",
    "    * (si.norm.cdf(d2(T, B**2 / (S0 * K))) - si.norm.cdf(d2(T, B / S0)))\n",
    "    )\n",
    "\n",
    "    return call\n",
    "\n",
    "def down_and_in_call(S0, K, T, r, sigma, BB):\n",
    "    \"\"\" \n",
    "    Compute a call price of a down and in barrier option (closed form solution)\n",
    "    \"\"\"\n",
    "\n",
    "    d1 = lambda t, s: 1 / (sigma * np.sqrt(t)) * (np.log(s) + (r + sigma**2 / 2) * t)\n",
    "    d2 = lambda t, s: 1 / (sigma * np.sqrt(t)) * (np.log(s) + (r - sigma**2 / 2) * t)\n",
    "\n",
    "    call = S0 * (BB / S0) ** (1 + 2 * r / sigma**2) * (si.norm.cdf(d1(T, BB**2 / (S0 * K)))) \n",
    "    - np.exp(-r * T) * K * (BB / S0) ** (-1 + 2 * r / sigma**2) * (si.norm.cdf(d2(T, BB**2 / (S0 * K))))\n",
    "\n",
    "    return call\n",
    "\n",
    "def create_data_example(n_points = 100, noise = False, case = 'UOB'):\n",
    "    \"\"\" \n",
    "    Returns:\n",
    "    a matrix containing the strike, the time to maturity, the volatility, the strike and the call price\n",
    "    in each column for 'n_points' number of rows\n",
    "    \"\"\"\n",
    "    \n",
    "    r = 0.0                                      # interest rate (not considered)    \n",
    "    np.random.seed(42)                           # fix the initial seed to obtain reproducable results\n",
    "    S = np.random.uniform(0.9,1.1, n_points)     # spot\n",
    "    K = np.random.uniform(0.6,1.4, n_points)     # strike\n",
    "    T = np.random.uniform(0.1,3.0, n_points)     # time to maturity\n",
    "    vol = np.random.uniform(0.05, 1.5, n_points) # volatility\n",
    "    B = np.random.uniform(0.8,1.2, n_points)       # Barrier \n",
    "    result = np.empty((n_points, 6))\n",
    "    for i in range(n_points):\n",
    "        result[i,0] = K[i]\n",
    "        result[i,1] = T[i]\n",
    "        result[i,2] = vol[i]\n",
    "        result[i,3] = S[i]\n",
    "        result[i,4] = B[i]\n",
    "        if case == 'UOB':\n",
    "            result[i,5] = up_and_out_call(S[i],K[i],T[i],r,vol[i], B[i]) \n",
    "        else: \n",
    "            result[i,5] = down_and_in_call(S[i],K[i],T[i],r,vol[i], B[i]) \n",
    "        if noise == True:\n",
    "            result[i,5] += 0.5*np.random.randn() # add N(0,1)-noise\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set should be split in a training and a test set, because the quality of the method is not based on the error produced with the data used for training, but it is based on the potential for generalisation of the algorithm, that is, the performance on data not previously seen. In an extreme case, overfitting can happen, such that the results for the given data are very good, but new data does not achieve an acceptable outcome. To measure this, a part of the data set should not be used for training the data, but for testing the algorithm after the training procedure.\n",
    "\n",
    "Here we create a data sample matrix with 10000 rows and use 80% as training data, while 20% are used for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10000\n",
    "data = create_data_example(size)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[:,0:-1], data[:,-1], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(x_train[:,0], bins=20, alpha=0.5, label='training')\n",
    "plt.hist(x_test[:,0], bins=20, alpha=0.5, label='test')\n",
    "plt.xlabel('strike')\n",
    "plt.ylabel('#points')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(x_train[:,1], bins=20, alpha=0.5, label='training')\n",
    "plt.hist(x_test[:,1], bins=20, alpha=0.5, label='test')\n",
    "plt.xlabel('time to maturity')\n",
    "plt.ylabel('#points')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(x_train[:,2], bins=20, alpha=0.5, label='training')\n",
    "plt.hist(x_test[:,2], bins=20, alpha=0.5, label='test')\n",
    "plt.xlabel('volatility')\n",
    "plt.ylabel('#points');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training the Neural Net\n",
    "\n",
    "In this section, we build and train the neural net by using the *sequential* keras model for a simple multi-layer net in the function *create_simple_network*.\n",
    "The function includes inner layers with the same activation function which can be specified and an output layer with a linear activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_network(n_neurons, activation='relu',\n",
    "                           kernel_regularizer=None, bias_regularizer=None, input_dim=1):\n",
    "\n",
    "    backend.clear_session()\n",
    "    np.random.seed(42)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons[0], activation=activation, input_dim=input_dim, \n",
    "                    kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)) #input layer\n",
    "    for n in n_neurons[1:]:\n",
    "        model.add(Dense(n, activation=activation, kernel_regularizer=kernel_regularizer, \n",
    "                    bias_regularizer=bias_regularizer)) \n",
    "    model.add(Dense(1, activation='linear')) #output layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now establish the neural network model with 3 hidden layers with 64 neurons each hidden layer. Moreover, we use the activation function *elu* for all neurons.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_simple_network((64,64,64), 'elu', input_dim=x_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be examined by calling the summary method on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "In this section, we train the neural net. The performance is significantly dependent on the optimizer, where the following two problems play a decisive role:\n",
    "- The optimizer finds only a local minimum which does not produce satisfying results for the neural network.\n",
    "- The optimizer converges very slowly, such that the user may wrongly assume that the minimum is reached without that being the case.\n",
    "\n",
    "Thus, it is crucial which optimizer and parameters are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a model checkpoint, i.e. during training the validation loss is checked and the model with best loss is saved\n",
    "cb = []\n",
    "#cb.append(callbacks.ModelCheckpoint('best_model.h5', save_best_only = True))\n",
    "\n",
    "# Set callback to log training progress in tensorboard (may be commented out)\n",
    "#log_dir = 'fit/logs/'+dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#cb.append(callbacks.TensorBoard(profile_batch=0, log_dir=log_dir, histogram_freq=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model to save computation time, may be commented out\n",
    "#model = keras.models.load_model('best_model.h5')\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='MSE')\n",
    "# Fit the model\n",
    "tf.random.set_seed(42) \n",
    "history = model.fit(x_train, y_train, epochs=2000, batch_size=200, verbose=0, callbacks=cb,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Model\n",
    "\n",
    "#### Training and Approximation Error\n",
    "\n",
    "We plot the convergence history of the neural net and the point-wise error between the analytical and the approximative solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_model(model, history, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_model(model, history, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Loss Function\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "The execution of the code in this section needs more computation time than the code in other sections.\n",
    "</div>\n",
    "\n",
    "To receive an impression of the training of the underlying minimisation problem and assess if the computed minimum is the correct solution, there exists the possibility to plot the cost functional in randomly sampled directions from the current point. One receives one-dimensional cuts of the high dimensional space (in different directions) which can also be plotted.\n",
    "For the current problem, such plots are depicted in the images down below, while the cutout is reduced from left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1,3,1)\n",
    "plot_loss_function(model, x_train, y_train, n_samples=40, start = -1.0, end = 1.0, stepsize=0.15)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plot_loss_function(model, x_test, y_test, n_samples=40, start = -0.001, end = 0.001, stepsize=0.00015)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plot_loss_function(model, x_test, y_test, n_samples=40, start = -0.0001, end = 0.0001, stepsize=0.000015);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "We compare the price with the results of the neural nets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the prices with respect to the input parameters \n",
    "# for the given model\n",
    "\n",
    "def compute_price(x, model):\n",
    "    x_ = Variable(x)\n",
    "    y = model(x_, training=False)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(K, vol, T,B):\n",
    "    S = np.linspace(0.8, 1.2, 50)\n",
    "    x = np.empty((S.shape[0],5))\n",
    "    x[:,0] = K\n",
    "    x[:,1] = T\n",
    "    x[:,2] = vol\n",
    "    x[:,3] = S\n",
    "    x[:,4] = B\n",
    "    price= compute_price(x, model)\n",
    "    \n",
    "    fig = plt.figure(figsize=(18,6))\n",
    "    price_closed = [up_and_out_call(S[i], K, T, 0.0, vol,B) for i in range(S.shape[0])]\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(S, price,'-x', label='NN')\n",
    "    plt.plot(S, price_closed,'-x', label='Closed Formula')\n",
    "    plt.xlabel('spot')\n",
    "    plt.ylabel('price')\n",
    "    plt.ylim([-0.5,0.5])\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interact(plot_errors, \n",
    "                 K=widgets.FloatSlider(value=1.0, min=0.6,max=1.4, step=0.02, continuous_update=False), \n",
    "                 vol=widgets.FloatSlider(value=0.2, min=0.05, max=0.8, step=0.01, continuous_update=False), \n",
    "                 T=widgets.FloatSlider(value=1.0, min=10.0/365.0, max=3.0, step=30.0/365.0, continuous_update=False),\n",
    "                 B=widgets.FloatSlider(value=1.0, min=0.8, max=1.2, step=0.02, continuous_update=False));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rivapyFS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
