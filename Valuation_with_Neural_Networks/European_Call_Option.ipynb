{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/logo.png\" alt=\"Your Image\" style=\"width: 500px; height: auto\"></center>\n",
    "\n",
    "# Introduction to approximation and fast evaluation of classical valuation methods via Neural Networks\n",
    "\n",
    "\n",
    "In this notebook, we show based on the example of option pricing how neural networks can be used for the approximation and fast evaluation of classical valuation methods. \n",
    "In the following, relevant principles are introduced with a focus on the design and training of neural networks in tensorflow.\n",
    "Thus, we limit ourselves to the simple case of a call price estimation with the Black-Scholes-Merton model defined as follows:\n",
    "\n",
    "The price $C$ of a call with maturity $T$, volatility $\\sigma$, interest rate $r$ and strike $K$ is given by the equation\n",
    "\n",
    "$$C=SN(d_1)-Ke^{-rT}N(d_2) $$\n",
    "\n",
    "with \n",
    "$$ d_1 = \\frac{1}{\\sigma\\sqrt{T}} \\left( \\ln \\left( \\frac{S}{K} \\right) + \\left( r + \\frac{\\sigma^2}{2} \\right) T \\right) \n",
    "\\quad \\text{and} \\quad \n",
    "d_2 = \\frac{1}{\\sigma\\sqrt{T}} \\left( \\ln \\left( \\frac{S}{K} \\right) + \\left( r - \\frac{\\sigma^2}{2} \\right) T \\right) = d_1 - \\sigma\\sqrt{T},$$\n",
    "\n",
    "where $N(d)$ denotes the standard normal cumulative distribution function $$N(d) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^d e^{-z^2/2} dz.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import math\n",
    "import ipywidgets as widgets\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as si\n",
    "import tensorflow as tf\n",
    "from tensorflow import Variable, GradientTape\n",
    "from tensorflow.keras import layers, regularizers, backend, callbacks, optimizers, losses, callbacks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, iter_min = 0, iter_max = -1):\n",
    "    \"\"\"Plot loss during network training on training and validation data\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['loss'][iter_min:iter_max],'-x', label='training')\n",
    "    plt.plot(history.history['val_loss'][iter_min:iter_max],'-x', label='validation')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('#iterations')\n",
    "    plt.ylabel('loss function value')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_loss_function(model, x, y, n_samples=30, random_state=42, start=-0.1, end=0.1, stepsize=0.001):\n",
    "    \"\"\"Plot projection of the network loss function along straight lines through the current network variables\n",
    "    This method may be helpful to get an impression if the training has slowed down due \n",
    "    to a saddlepoint or maybe because a real local minima has been reached\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    weights = [ np.copy(x) for x in model.get_weights() ]\n",
    "    \n",
    "    steps = np.arange(start,end,stepsize)\n",
    "    loss_values = np.empty(steps.shape)\n",
    "    for sample in range(n_samples):\n",
    "        direction = []\n",
    "        for w in weights:\n",
    "            direction.append( np.random.uniform(0, 1, size = w.shape) )\n",
    "        for i in range(steps.shape[0]):\n",
    "            w = []\n",
    "            for j in range(len(weights)):\n",
    "                new_weights = np.copy(weights[j])\n",
    "                new_weights += steps[i]*direction[j]\n",
    "                w.append(new_weights)\n",
    "            model.set_weights(w)\n",
    "            loss_values[i] = model.evaluate(x,y, verbose=0)\n",
    "        plt.plot(steps, loss_values)\n",
    "    model.set_weights(weights)  \n",
    "    plt.ylabel('loss')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.axvline(x=0.0)\n",
    "      \n",
    "def analyze_model_BS(model, history, x, y):\n",
    "    \"\"\"This method plots the approximation errors on training and test data set.\n",
    "    \"\"\"\n",
    "    print('Loss function on training data: '+ str(history.history['loss'][-1]))\n",
    "    print('Loss function on validation data: '+ str(history.history['val_loss'][-1]))\n",
    "\n",
    "    y_pred = model.predict(x).squeeze()\n",
    "    \n",
    "    plt.subplots(figsize=(25,12))\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.subplot(2,2,1)\n",
    "    plot_loss(history)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()    \n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(x[:,0], y-y_pred, '.', label='y-y_pred')\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('y-y_pred')\n",
    "    plt.tight_layout()\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.plot(x[:,1], y-y_pred, '.', label='y-y_pred')\n",
    "    plt.xlabel('T')\n",
    "    plt.ylabel('y-y_pred')\n",
    "    plt.tight_layout()\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.plot(x[:,1], y-y_pred, '.', label='y-y_pred')\n",
    "    plt.xlabel('vol')\n",
    "    plt.ylabel('y-y_pred')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and plot data\n",
    "\n",
    "We create input data for the training of the neural network, including\n",
    "- strike\n",
    "- time to maturity\n",
    "- volatility\n",
    "- spot \n",
    "\n",
    "We will not consider interest rates or dividends, as they are not necessary under the [Buehler](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1141877 \"Volatility and Dividends (2010)\") model. The parameters are randomly drawn from a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euro_vanilla_call(S, K, T, r, sigma):\n",
    "    \"\"\" \n",
    "    Compute a European call price based on the Black Scholes model\n",
    "    \"\"\"\n",
    "    \n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = (np.log(S / K) + (r - 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))  \n",
    "    call = (S * si.norm.cdf(d1, 0.0, 1.0) - K * np.exp(-r * T) * si.norm.cdf(d2, 0.0, 1.0))\n",
    "    \n",
    "    return call\n",
    "\n",
    "def create_data_example(n_points = 100, noise = False):\n",
    "    \"\"\" \n",
    "    Returns:\n",
    "    a matrix containing the strike, the time to maturity, the volatility, the strike and the call price\n",
    "    in each column for 'n_points' number of rows\n",
    "    \"\"\"\n",
    "    \n",
    "    r = 0.0                                      # interest rate (not considered)    \n",
    "    np.random.seed(42)                           # fix the initial seed to obtain reproducable results\n",
    "    S = np.random.uniform(0.9,1.1, n_points)     # spot\n",
    "    K = np.random.uniform(0.6,1.4, n_points)     # strike\n",
    "    T = np.random.uniform(0.1,3.0, n_points)     # time to maturity\n",
    "    vol = np.random.uniform(0.05, 1.5, n_points) # volatility\n",
    "    result = np.empty((n_points, 5))\n",
    "    for i in range(n_points):\n",
    "        result[i,0] = K[i]\n",
    "        result[i,1] = T[i]\n",
    "        result[i,2] = vol[i]\n",
    "        result[i,3] = S[i]\n",
    "        result[i,4] = euro_vanilla_call(S[i],K[i],T[i],r,vol[i]) \n",
    "        if noise == True:\n",
    "            result[i,4] += 0.5*np.random.randn() # add N(0,1)-noise\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set should be split in a training and a test set, because the quality of the method is not based on the error produced with the data used for training, but it is based on the potential for generalisation of the algorithm, that is, the performance on data not previously seen. In an extreme case, overfitting can happen, such that the results for the given data are very good, but new data does not achieve an acceptable outcome. To measure this, a part of the data set should not be used for training the data, but for testing the algorithm after the training procedure.\n",
    "\n",
    "Here we create a data sample matrix with 5000 rows and use 80% as training data, while 20% are used for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 5000\n",
    "data = create_data_example(size)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[:,0:-1], data[:,-1], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(x_train[:,0], bins=20, alpha=0.5, label='training')\n",
    "plt.hist(x_test[:,0], bins=20, alpha=0.5, label='test')\n",
    "plt.xlabel('strike')\n",
    "plt.ylabel('#points')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(x_train[:,1], bins=20, alpha=0.5, label='training')\n",
    "plt.hist(x_test[:,1], bins=20, alpha=0.5, label='test')\n",
    "plt.xlabel('time to maturity')\n",
    "plt.ylabel('#points')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(x_train[:,2], bins=20, alpha=0.5, label='training')\n",
    "plt.hist(x_test[:,2], bins=20, alpha=0.5, label='test')\n",
    "plt.xlabel('volatility')\n",
    "plt.ylabel('#points');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training the Neural Net\n",
    "\n",
    "In this section, we build and train the neural net by using the *sequential* keras model for a simple multi-layer net in the function *create_simple_network*.\n",
    "The function includes inner layers with the same activation function which can be specified and an output layer with a linear activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_network(n_neurons, activation='relu',\n",
    "                           kernel_regularizer=None, bias_regularizer=None, input_dim=1):\n",
    "\n",
    "    backend.clear_session()\n",
    "    np.random.seed(42)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons[0], activation=activation, input_dim=input_dim, \n",
    "                    kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)) #input layer\n",
    "    for n in n_neurons[1:]:\n",
    "        model.add(Dense(n, activation=activation, kernel_regularizer=kernel_regularizer, \n",
    "                    bias_regularizer=bias_regularizer)) \n",
    "    model.add(Dense(1, activation='linear')) #output layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now establish the neural network model with 2 hidden layers with 20 neurons in the first hidden layer and 10 neurons in the second hidden layer. Moreover, we use the activation function *elu* for all neurons.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_simple_network((20,10), 'elu', input_dim=x_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be examined by calling the summary method on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "In this section, we train the neural net. The performance is significantly dependent on the optimizer, where the following two problems play a decisive role:\n",
    "- The optimizer finds only a local minimum which does not produce satisfying results for the neural network.\n",
    "- The optimizer converges very slowly, such that the user may wrongly assume that the minimum is reached without that being the case.\n",
    "\n",
    "Thus, it is crucial which optimizer and parameters are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a model checkpoint, i.e. during training the validation loss is checked and the model with best loss is saved\n",
    "cb = []\n",
    "#cb.append(callbacks.ModelCheckpoint('best_model.h5', save_best_only = True))\n",
    "\n",
    "# Set callback to log training progress in tensorboard (may be commented out)\n",
    "#log_dir = 'fit/logs/'+dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#cb.append(callbacks.TensorBoard(profile_batch=0, log_dir=log_dir, histogram_freq=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model to save computation time, may be commented out\n",
    "#model = keras.models.load_model('best_model.h5')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.0005), loss='MSE')\n",
    "# Fit the model\n",
    "tf.random.set_seed(42) \n",
    "history = model.fit(x_train, y_train, epochs=2000, batch_size=100, verbose=0, callbacks=cb,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Model\n",
    "\n",
    "#### Training and Approximation Error\n",
    "\n",
    "We plot the convergence history of the neural net and the point-wise error between the analytical and the approximative solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_model_BS(model, history, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do the same analysis on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_model_BS(model, history, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Loss Function\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "The execution of the code in this section needs more computation time than the code in other sections.\n",
    "</div>\n",
    "\n",
    "To receive an impression of the training of the underlying minimisation problem and assess if the computed minimum is the correct solution, there exists the possibility to plot the cost functional in randomly sampled directions from the current point. One receives one-dimensional cuts of the high dimensional space (in different directions) which can also be plotted.\n",
    "For the current problem, such plots are depicted in the images down below, while the cutout is reduced from left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1,3,1)\n",
    "plot_loss_function(model, x_train, y_train, n_samples=40, start = -1.0, end = 1.0, stepsize=0.15)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plot_loss_function(model, x_test, y_test, n_samples=40, start = -0.001, end = 0.001, stepsize=0.00015)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plot_loss_function(model, x_test, y_test, n_samples=40, start = -0.0001, end = 0.0001, stepsize=0.000015);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the learning rate\n",
    "To get a feeling for the training behavior, the training and subsequent analysis may be conducted for various learning rates (parameter *lr*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # for execution change False to True. But be careful: 'the last 42 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f35f421c430> triggered tf.function retracing'. Tracing is expensive: This may significantly increase execution time. ;-)\n",
    "    lr = [0.1, 0.01, 0.001, 0.0001]\n",
    "    for l in lr:\n",
    "        model.compile(optimizer=keras.optimizers.Adam(lr=l), loss='MSE')\n",
    "        tf.random.set_seed(42)\n",
    "        history = model.fit(x_train, y_train, epochs=10, batch_size=100, verbose=0, callbacks=cb,\n",
    "                        validation_split=0.2)\n",
    "        analyze_model_BS(m, history, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting Example with a Learning Rate Scheduler\n",
    "\n",
    "In the following, we show a quick example of an overfitting situation, where the data is fit well on the training data, but not on the validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100\n",
    "data = create_data_example(size, noise=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[:,0:-1], data[:,-1], test_size=0.2, random_state=42)\n",
    "overfit_model = create_simple_network((30,30,30), 'elu', input_dim=x_train.shape[1])\n",
    "overfit_model.compile(optimizer=optimizers.Adam(lr=0.0005), loss=losses.MSE)\n",
    "\n",
    "history = overfit_model.fit(x_train, y_train, epochs=12000, batch_size=20, verbose=0, validation_split=0.2)\n",
    "analyze_model_BS(overfit_model, history, x_train, y_train)\n",
    "#analyze_model_BS(model, history, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate Scheduler\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "The execution of the code in this section needs more computation time than the code in other sections due to the number of epochs.\n",
    "</div>\n",
    "\n",
    "As the loss function value begins to oscillate with an increasing number of iterations, a learning rate scheduler can be introduced to reduce the learning rate over time in order to improve on convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_scheduler(choose_schedule: str):\n",
    "    \n",
    "    if choose_schedule == 'learning_rate_time_decay'  :\n",
    "        def learning_rate_time_decay(epoch, lr): \n",
    "            return lr * 1 / (1 + decay * epoch)\n",
    "        learning_rate_scheduler = callbacks.LearningRateScheduler(learning_rate_time_decay)\n",
    "\n",
    "    elif choose_schedule == 'learning_rate_step_decay':\n",
    "        drop_rate = 0.5\n",
    "        epochs_drop = 100\n",
    "        def learning_rate_step_decay(epoch, lr):\n",
    "            return lr * math.pow(drop_rate, math.floor(epoch/epochs_drop))\n",
    "        learning_rate_scheduler = callbacks.LearningRateScheduler(learning_rate_step_decay)\n",
    "\n",
    "    elif choose_schedule == 'learning_rate_exp_decay':\n",
    "        k = 0.1\n",
    "        def learning_rate_exp_decay(epoch, lr):\n",
    "            if epoch > 0 and epoch%100==0: return lr * math.exp(-k*epoch)\n",
    "            else: return lr\n",
    "        learning_rate_scheduler = callbacks.LearningRateScheduler(learning_rate_exp_decay)\n",
    "\n",
    "    else:\n",
    "        print('Learning rate scheduler unknown! None chosen.')\n",
    "        learning_rate_scheduler = []\n",
    "    \n",
    "    return learning_rate_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.0005\n",
    "num_of_epochs = 60000\n",
    "decay = initial_learning_rate / num_of_epochs \n",
    "choose_schedule = 'learning_rate_time_decay'\n",
    "\n",
    "size = 100\n",
    "data = create_data_example(size, noise=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[:,0:-1], data[:,-1], test_size=0.2, random_state=42)\n",
    "overfit_model = create_simple_network((30,30,30), 'elu', input_dim=x_train.shape[1])\n",
    "overfit_model.compile(optimizer=optimizers.Adam(lr=initial_learning_rate), loss=losses.MSE)\n",
    "\n",
    "history_decay = overfit_model.fit(x_train, y_train, epochs=num_of_epochs, batch_size=20, \n",
    "                                             verbose=0, validation_split=0.2, \n",
    "                                             callbacks=[learning_rate_scheduler(choose_schedule)])\n",
    "\n",
    "analyze_model_BS(overfit_model, history_decay, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greeks - Algorithmic Differentiation (AD)\n",
    "\n",
    "Most frameworks for machine learning offer the opportunity to differentiate the model with respect to specific model parameters by automatic differentiation (**AD** or **AAD**).\n",
    "This is based on the fact that for the training of the neural net with a stochastic gradient method, the appropriate derivatives of the cost functional with respect to the parameters of neural nets are needed.\n",
    "\n",
    "This can be used to calculate the derivatives of the input parameters (in finance often called **Greeks**).\n",
    "Tensorflow offers a simple possibility with *GradientTapes*, which we use to calculate the derivative of the option price with respect to the spot (**Delta**) and the volatility (**Vega**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the prices and derivatives with respect to the input parameters \n",
    "# for the given model with automatic differentiation\n",
    "\n",
    "def compute_price_delta(x, model):\n",
    "    x_ = Variable(x)\n",
    "    with GradientTape() as g:\n",
    "        g.watch(x_)\n",
    "        y = model(x_, training=False)\n",
    "    return y, g.gradient(y, x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "We compare the price as well as the delta and vega of the Black-Scholes-Merton model with the results of the neural nets. Note that the neural network resulting from the above training using the default parameters performs poorly. Play around (especially with number of epochs and the learning rate) to enhance the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(K, vol, T):\n",
    "    S = np.linspace(0.8, 1.2, 50)\n",
    "    x = np.empty((S.shape[0],4))\n",
    "    x[:,0] = K\n",
    "    x[:,1] = T\n",
    "    x[:,2] = vol\n",
    "    x[:,3] = S\n",
    "    price, deriv = compute_price_delta(x, model)\n",
    "    \n",
    "    fig = plt.figure(figsize=(18,6))\n",
    "    price_bs = [euro_vanilla_call(S[i], K, T, 0.0, vol) for i in range(S.shape[0])]\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(S, price,'-x', label='NN')\n",
    "    plt.plot(S, price_bs,'-x', label='Black-Scholes')\n",
    "    plt.xlabel('spot')\n",
    "    plt.ylabel('price')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    delta_bs = [(euro_vanilla_call(S[i]+0.001, K, T, 0.0, vol)-price_bs[i])/0.001 for i in range(S.shape[0])]\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(S, deriv[:,3],'-x', label='NN')\n",
    "    plt.plot(S, delta_bs,'-x', label='Black-Scholes')\n",
    "    plt.xlabel('spot')\n",
    "    plt.ylabel('delta')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    vega_bs = [(euro_vanilla_call(S[i], K, T, 0.0, vol+0.001)-price_bs[i])/0.001 for i in range(S.shape[0])]\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(S, deriv[:,2],'-x', label='NN')\n",
    "    plt.plot(S, vega_bs,'-x', label='Black-Scholes')\n",
    "    plt.xlabel('spot')\n",
    "    plt.ylabel('vega')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interact(plot_errors, \n",
    "                 K=widgets.FloatSlider(value=1.0, min=0.6,max=1.4, step=0.02, continuous_update=False), \n",
    "                 vol=widgets.FloatSlider(value=0.2, min=0.05, max=0.8, step=0.01, continuous_update=False), \n",
    "                 T=widgets.FloatSlider(value=1.0, min=10.0/365.0, max=3.0, step=30.0/365.0, continuous_update=False));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rivapyFS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
