{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/logo.png\" alt=\"Your Image\" style=\"width: 500px; height: auto\"></center>\n",
    "\n",
    "# Introduction to Echo State Networks based on the Example of Stock Price Prediction\n",
    "\n",
    "In this notebook, we show how echo state networks (ESN) can be used for time series prediction and especially the prediction of stock price dynamics. In the following, the basic model is introduced with a focus on the design and hyperparameterization of echo state networks in python. Here, for the sake of simplicity, we limit ourselves to the case of a single stock prices time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and plot data \n",
    "\n",
    "We collected stock data for Apple on 880 trading days, from January 1, 2020, to July 1, 2023. These stock data includes volumes and opening, closing,\n",
    "highest, lowest, and adjusted closing prices. We load data using the Python package Yahoo Finance (yfinance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = yf.download(\"AAPL\", start=\"2020-01-01\", end=\"2023-07-15\")\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the collected closing prices, which are the volume-weighted average prices of all trades (including the last trade) within the last\n",
    "minute of trading and are important for stock price forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_prices = stock_data['Close']\n",
    "\n",
    "plt.figure(figsize=[15,5])\n",
    "plt.plot(close_prices)\n",
    "plt.title('Daily closing prices of chosen stock')\n",
    "plt.ylabel('Closing price')\n",
    "plt.xlabel('Day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test set\n",
    "\n",
    "\n",
    "The data set should be split in a training and a test set, because the quality of the method is not based on the error produced with the data used for training, but it is based on the potential for generalisation of the algorithm, that is, the performance on data not previously seen. In an extreme case, overfitting can happen, such that the results for the given data are very good, but new data does not achieve an acceptable outcome. To measure this, a part of the data set should not be used for training the data, but for testing the algorithm after the training procedure.\n",
    "\n",
    "The task here is to predict 2 days ahead by using the previous 800 days (training data) and do that for 80 future points (test data). So, in the end you will have a 80 time step prediction with dif = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 800\n",
    "test_len = 80\n",
    "dif = 2  # prediction horizon >=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(close_prices)\n",
    "X = data[0:train_len] \n",
    "y = data[dif:train_len+dif]\n",
    "Xtest = data[train_len:train_len+test_len]\n",
    "ytest = data[train_len+dif:train_len+test_len+dif]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training the Echo State Network\n",
    "\n",
    "In this section, we build and train the echo state network (based on https://www.ai.rug.nl/minds/uploads/PracticalESN.pdf and https://github.com/alexander-rakhlin/ESN). \n",
    "\n",
    "\n",
    "Echo State Networks are a simple type of recurrent neural networks (RNN) which utilize reservoir computation.\n",
    "\n",
    "The following section from [geeksforgeeks.org](https://www.geeksforgeeks.org/echo-state-network-an-overview/) states easy way to understand the basic concept of ESNs:\n",
    "\n",
    "\"Imagine an Echo State Network (ESN) in Python as a smart musical instrument player. Let’s say you’re trying to teach the player to mimic a song. The player has a large collection of notes (reservoir), and when you play the first few notes of the song (input), the player responds with its interpretation of the melody.\n",
    "Now, here’s the trick: the player’s ability to interpret the song is based on its own unique style (randomly initialized reservoir), and it doesn’t change its playing style during practice (fixed weights). As you play more notes, the player adjusts how it combines those notes to match the song’s rhythm and melody. So, the player becomes really good at predicting the next note in the song, even if it hasn’t heard that specific sequence before.\"\n",
    "\n",
    "\n",
    "The standard ESN structure includes input data (i.e., stock prices, $\\mathbf{u}(n)$), reservoir activation states, and network output ($\\mathbf{y}(n)$). The main difference between an ESN and a 'traditional' neural network is that the input and recurrent weight matrices $\\mathbf{W}_{in}$ and $\\mathbf{W}_{res}$ are initilized and set randomly when the network is established, such that little training is required. Solely, the output weight matrix $\\mathbf{W}_{out}$ is trained. Moreover, the typical update equations are\n",
    "\n",
    "$\\tilde{x}(n) = \\mathrm{tanh} (\\mathbf{W}_{in} + \\mathbf{W}_{res} \\mathbf{x}(n-1))$, \n",
    "\n",
    "$\\mathbf{x}(n) = (1-\\alpha) \\mathbf{x}(n-1) + \\alpha \\mathbf{\\tilde{x}}(n)$ ,\n",
    "\n",
    "where $x(n)$ is a vector of reservoir neuron activations, $\\mathbf{\\tilde{x}}(n)$ is its update at timestep $n$ and $\\alpha$ describes the leaking rate, which can be regarded as the speed of the reservoir update dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "      \n",
    "class ESN(object):\n",
    "\n",
    "    def __init__(self, res_size=500, rho=0.9, cr=0.05, leaking_rate=0.2, W=None):\n",
    "\n",
    "        self.res_size = res_size\n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.rho = rho\n",
    "        self.cr = cr\n",
    "\n",
    "        if W is None:\n",
    "            N = self.res_size * self.res_size\n",
    "            W = np.random.rand(N) - 0.5\n",
    "            zero_index = np.random.permutation(N)[int(N * self.cr * 1.0):]\n",
    "            W[zero_index] = 0\n",
    "            W = W.reshape((self.res_size, self.res_size))\n",
    "            rhoW = max(abs(linalg.eig(W)[0]))\n",
    "            W *= self.rho / rhoW\n",
    "        else:\n",
    "            assert W.shape[0] == W.shape[1] == self.res_size, \"reservoir size mismatch\"\n",
    "        self.W = W\n",
    "\n",
    "    def __init_states__(self, X, init_len, reset_state=True):\n",
    "\n",
    "        self.S = np.zeros((len(X) - init_len, 1 + self.inSize + self.res_size))\n",
    "        if reset_state:\n",
    "            self.s = np.zeros(self.res_size)\n",
    "        s = self.s.copy()\n",
    "\n",
    "        for t, u in enumerate(X):\n",
    "            s = (1 - self.leaking_rate) * s + self.leaking_rate *\\\n",
    "                                np.tanh(np.dot(self.Win, np.hstack((1, u))) +\\\n",
    "                                np.dot(self.W, s))\n",
    "            if t >= init_len:\n",
    "                self.S[t-init_len] = np.hstack((1, u, s))\n",
    "        if reset_state:\n",
    "            self.s = s\n",
    "\n",
    "    def fit(self, X, y, lmbd=1e-6, init_len=100, init_states=True):\n",
    "        assert len(X) == len(y), \"input lengths mismatch.\"\n",
    "        self.inSize =  1 if np.ndim(X) == 1 else X.shape[1]\n",
    "        if init_states:\n",
    "            self.Win = (np.random.rand(self.res_size, 1 + self.inSize) - 0.5) * 1\n",
    "            self.__init_states__(X, init_len)\n",
    "        self.ridge = Ridge(alpha=lmbd, fit_intercept=False,\n",
    "                               solver='svd', tol=1e-6)\n",
    "        self.ridge.fit(self.S, y[init_len:])\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, init_states=True):\n",
    "        if init_states:        \n",
    "            self.__init_states__(X, 0, reset_state=False)\n",
    "        y = self.ridge.predict(self.S)\n",
    "        return y\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now establish the model with a reservoir size of 100 and set the hyperparameters to: \n",
    "\n",
    "- a spectral radius (rho) of 0.999\n",
    "- a connectivity ratio (cr) of 0.02\n",
    "- a leaking rate of 1\n",
    "- a regularization coefficient (lmbd) of 1e-6\n",
    "\n",
    "Then, the ESN is trained. Note, that the performance is significantly dependent on the set hyperparameters. General rules of thumb, which should be considered when setting hyperparameters in ESNs are published in \"A Practical Guide to Applying Echo State Networks\" by Lukosevicius (2012). Here, we concentrate on the following three key parameters: reservoir size, spectral radius, and the connectivity ratio of reservoirs. The reservoir size represents the number of neurons in the pool, is generally related to the number of samples and it has a certain effect on the network performance. A larger size means a more accurate stock forecast (however, keep in mind overfitting), while a too small size leads to underfitting. If the spectral radius of $W_{res}$ is less than 1, ESN will stay in an echo state (cf. Lukosevicius, 2012).  The connectivity ratio of a reservoir represents the connections between its neurons, meaning the more connections the stronger the nonlinear approximation ability (also cf. https://pdfs.semanticscholar.org/e3a7/2a2b7d6071461007112f12c8716529755d39.pdf).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_size = 500\n",
    "rho = 0.999\n",
    "cr = 0.02\n",
    "leaking_rate = 1.\n",
    "lmbd = 1e-6\n",
    "\n",
    "esn = ESN(res_size=res_size, rho=rho, cr=cr, leaking_rate=leaking_rate)\n",
    "    \n",
    "esn.fit(X, y, init_len=100, lmbd=lmbd)\n",
    "y_predicted = esn.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the model\n",
    "\n",
    "We plot the training and test data to the predicted data of the ESN and evaluated the performance of ESN based on the root mean square error (RMSE) of forecast values ($y_t^p$) and actual values ($y_t$) at time $t$ with a total number of forecast values $N$:\n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{N} \\sum_{t=1}^N ( y_t - y_t^p)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(0,train_len+test_len),data[0:train_len+test_len],'b',alpha =0.5,label=\"Data\")\n",
    "plt.plot(range(train_len+dif,train_len+test_len+dif),y_predicted,'red', label=\"ESN\")\n",
    "plt.legend(loc='upper left',fontsize='x-large')\n",
    "plt.title('Prediction window = 2')\n",
    "plt.ylabel('Closing price')\n",
    "plt.xlabel('Day')\n",
    "lo,hi = plt.ylim()\n",
    "plt.plot([train_len+dif,train_len+dif],[lo+np.spacing(1),hi-np.spacing(1)],'k:', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(np.sum(np.square(ytest-y_predicted))/test_len)\n",
    "print('RMSE:', RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows, that the method works quite well, but keep in mind, that the above model was made with a prediction window of two days, meaning that we are only ever predicting 2 days into the future at any given time. \n",
    "\n",
    "Next, we analyse the dependency of the RMSE with regard to the prediction window length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSME as function of prediction window length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = []\n",
    "wl = [1,2,3,4,5,6,7,8]\n",
    "\n",
    "for i in wl:\n",
    "    dif = i  \n",
    "    data = np.array(close_prices)\n",
    "    y = data[dif:train_len+dif]\n",
    "    ytest = data[train_len+dif:train_len+test_len+dif]\n",
    "    esn = ESN(res_size=res_size, rho=rho, cr=cr, leaking_rate=leaking_rate)\n",
    "    esn.fit(X, y, init_len=100, lmbd=lmbd)\n",
    "    y_predicted = esn.predict(Xtest)\n",
    "\n",
    "\n",
    "    RMSE.append(np.sqrt(np.sum(np.square(ytest-y_predicted))/test_len))\n",
    "\n",
    "plt.plot(wl,RMSE)\n",
    "plt.xlabel('prediction window length')\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future predictions, the error increases in time such that as the window length is increased the accuracy decreases. We can see this behavior in the plot above, where longer predictions show a larger RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the reservoir size\n",
    "\n",
    "To get a feeling for the hyperparameters, the training may be conducted for various reservoir sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = []\n",
    "res_size = [100,200,500,1000]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "for i in res_size:\n",
    "    dif = 2\n",
    "    data = np.array(close_prices)\n",
    "    esn = ESN(res_size=i, rho=rho, cr=cr, leaking_rate=leaking_rate)\n",
    "    esn.fit(X, y, init_len=100, lmbd=lmbd)\n",
    "    y_predicted = esn.predict(Xtest)\n",
    "\n",
    "    plt.plot(range(train_len+dif,train_len+test_len+dif),y_predicted, label=\"ESN: res_size=\"+str(i))\n",
    "\n",
    "    RMSE.append(np.sqrt(np.sum(np.square(ytest-y_predicted))/test_len))\n",
    "\n",
    "plt.legend(loc='upper left',fontsize='x-large')\n",
    "plt.title('Prediction window = 2')\n",
    "plt.ylabel('Closing price')\n",
    "plt.xlabel('Day')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rivapyFS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
