{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/logo.png\" alt=\"Your Image\" style=\"width: 500px; height: auto\"></center>\n",
    "\n",
    "# Introduction to Recurrent Neural Networks based on the Example of Stock Price Prediction\n",
    "\n",
    "\n",
    "A RNN is a type of artificial neural network designed for sequential data processing, where the current input is related to previous inputs. This characteristic makes RNNs particularly well-suited for tasks involving time series or sequences, such as financial forecasting. In this notebook, we show how a simple recurrent neural network (RNN) model can be used for the prediction of stock price dynamics. Thereby, relevant principles are introduced with a focus on the design and training of a simple RNN in tensorflow. For simplicity, limit ourselves to the simple case of a single stock price time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and plot data \n",
    "\n",
    "We collected stock data for Apple on 880 trading days, from January 1, 2020, to July 1, 2023. These stock data includes volumes and opening, closing,\n",
    "highest, lowest, and adjusted closing prices. We load data using the Python package Yahoo Finance (yfinance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = yf.download(\"AAPL\", start=\"2020-01-01\", end=\"2023-07-15\")\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of Data and Split the Training and Test Set\n",
    "\n",
    "Since RNNs are sensitive to the scale of input data due to their dynamic temporal behavior, it is often helpful to normalize the time series data (e.g., to mitigate issues related to exploding or vanishing gradients).\n",
    "\n",
    "Moreover, the data set should be split in a training and a test set, because the quality of the method is not based on the error produced with the data used for training, but it is based on the potential for generalisation of the algorithm, that is, the performance on data not previously seen. In an extreme case, overfitting can happen, such that the results for the given data are very good, but new data does not achieve an acceptable outcome. To measure this, a part of the data set should not be used for training the data, but for testing the algorithm after the training procedure.\n",
    "\n",
    "As an exercise, we split here the data manually. However, note that the split can be done in a shorter way via the *sklearn* method *train_test_split*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = stock_data['Close']\n",
    "\n",
    "length_data = len(data)     \n",
    "split_ratio = 0.8           \n",
    "length_train = round(length_data * split_ratio)  \n",
    "length_validation = length_data - length_train\n",
    "\n",
    "train_data = data[:length_train]\n",
    "dataset_train = np.array(data[:length_train])\n",
    "dataset_train = np.reshape(dataset_train, (-1,1))\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "dataset_train_scaled = scaler.fit_transform(dataset_train)\n",
    "dataset_train_scaled.shape\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "time_step = 50\n",
    "for i in range(time_step, length_train):\n",
    "    X_train.append(dataset_train_scaled[i-time_step:i,0])\n",
    "    y_train.append(dataset_train_scaled[i,0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))\n",
    "y_train = np.reshape(y_train, (y_train.shape[0],1))\n",
    "\n",
    "validation_data = data[length_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,5])\n",
    "plt.plot(dataset_train_scaled)\n",
    "plt.title('Daily closing prices of chosen stock (training dataset)')\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Normalized Closing Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training a simple RNN\n",
    "\n",
    "In this section, we build and train a simple RNN model ([Wiki about RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network), [Introduction to RNN](https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/)) by using the sequential keras model for a simple recurrent network in the function create_simple_RNN, which includes inner layers with the same activation function which can be specified and an output layer. Like other neural networks, RNNs consist of nodes organized into layers. However, RNNs have an additional loop mechanism that allows information to persist. In particular, each node in an RNN is not only connected to the nodes in the next layer but also has connections to itself from the previous time step (i.e., the units are recurrently connected).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_RNN(units, activation='tanh',return_sequences=True,input_shape=(X_train.shape[1],1)):\n",
    "\n",
    "    # initializing the RNN\n",
    "    model = Sequential()\n",
    "    # adding input layer, RNN layers and dropout regulatizations\n",
    "    model.add(SimpleRNN(units=units, activation=activation, return_sequences=return_sequences, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(SimpleRNN(units=units, activation=activation, return_sequences=return_sequences))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(SimpleRNN(units=units, activation=activation, return_sequences=return_sequences))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(SimpleRNN(units=units))\n",
    "    model.add(Dropout(0.2))\n",
    "    # adding output layer\n",
    "    model.add(Dense(units = 1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing the model\n",
    "\n",
    "We now establish the model with four hidden layers\n",
    "\n",
    "- 50 neurons in each layer and the activation function 'tanh' for all neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_simple_RNN(units=50, activation='tanh', input_shape=(X_train.shape[1],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be examined by calling the summary method on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "In this section, we train the simple RNN. The performance is significantly dependent on the optimizer, where the following two problems play a decisive role:\n",
    "\n",
    "- The optimizer finds only a local minimum which does not produce satisfying results for the neural network.\n",
    "- The optimizer converges very slowly, such that the user may wrongly assume that the minimum is reached without that being the case.\n",
    "\n",
    "Thus, it is crucial which optimizer and parameters are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.0005), loss = \"mean_squared_error\",metrics = [\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs = 50, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Model\n",
    "\n",
    "### Training and approximation error\n",
    "\n",
    "We do a comparison between the original time series (training data) and the predicted solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train) \n",
    "\n",
    "plt.figure(figsize=[15,5])\n",
    "plt.plot(y_pred, color = \"b\", label = \"y_pred\" )\n",
    "plt.plot(y_train, color = \"g\", label = \"y_train\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Closing price\")\n",
    "plt.title(\"Predictions with normalized input X_train vs y_train\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do the same analysis on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = validation_data.values  \n",
    "dataset_validation = np.reshape(dataset_validation, (-1,1))  \n",
    "scaled_dataset_validation =  scaler.fit_transform(dataset_validation) \n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(time_step, length_validation):\n",
    "    X_test.append(scaled_dataset_validation[i-time_step:i,0])\n",
    "    y_test.append(scaled_dataset_validation[i,0])\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "y_test = np.reshape(y_test, (-1,1)) \n",
    "\n",
    "\n",
    "y_pred_of_test = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=[15,5])\n",
    "plt.plot(y_pred_of_test, color = \"b\", label = \"y_pred_of_test\" )\n",
    "plt.plot(y_test, color = \"g\", label = \"y_train\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Closing price\")\n",
    "plt.title(\"Prediction with input X_test vs y_test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the loss function\n",
    "\n",
    "To receive an impression of the training of the underlying minimisation problem and assess if the computed minimum is the correct solution, there exists the possibility to plot the loss function vs. epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loss function on training data: '+ str(history.history['loss'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,7))\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.title(\"Simple RNN model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the learning rate\n",
    "\n",
    "To get a feeling for the training behavior, the training and subsequent analysis may be conducted for various learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "loss_list = []\n",
    "plt.figure(figsize=[15,5])\n",
    "plt.plot(y_train, color = \"k\", label = \"y_train\")\n",
    "lr = [0.001,0.0001,0.00001]\n",
    "for l in lr:\n",
    "    model = create_simple_RNN(units=50, activation='tanh', input_shape=(X_train.shape[1],1))\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=l), loss = \"mean_squared_error\",metrics = [\"accuracy\"])\n",
    "    tf.random.set_seed(42)\n",
    "    history = history = model.fit(X_train, y_train, epochs = 50, batch_size = 32)\n",
    "    loss_list.append(history.history['loss'][-1])\n",
    "    y_pred = model.predict(X_train)\n",
    "    plt.plot(y_pred, label = \"y_pred for lr=\"+str(l) )\n",
    "\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Closing price\")\n",
    "plt.title(\"Predictions with normalized input X_train vs y_train\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lr,loss_list)\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('training loss')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rivapyFS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
